{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de4f56d9-4038-482f-b1a0-fba34b4df702",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Databricks notebook source \n",
    "\t•\treads approved SQL from ai_sql_candidates,\n",
    "\t•\tcreates/updates the UC view from that approved SQL,\n",
    "\t•\twrites _DRAFT artifacts (CSV / XLSX, PDF optional) to DBFS,\n",
    "\t•\twrites a manifest per report,\n",
    "\t•\tupserts into report_candidates with status='ready_for_business'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46db0b77-d69f-46e4-b851-e7ab8d39b047",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Config & helpers\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "CATALOG        = \"finance\"\n",
    "SCHEMA         = \"kyc_gold\"\n",
    "META_FILE      = \"dbfs:/FileStore/kyc/report_metadata/report_definitions.json\"\n",
    "VIEW_DUMP_ROOT = \"dbfs:/FileStore/kyc/reports/views\"\n",
    "AI_CANDIDATES_VIEW = f\"{CATALOG}.{SCHEMA}.ai_sql_candidates\"\n",
    "\n",
    "# ---------- IMPORTS ----------\n",
    "import os, io, json, uuid, hashlib\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.getActiveSession()\n",
    "assert spark is not None, \"Must run on a Databricks cluster\"\n",
    "\n",
    "# PDF is optional\n",
    "try:\n",
    "    from reportlab.lib.pagesizes import A4, landscape\n",
    "    from reportlab.lib import colors\n",
    "    from reportlab.lib.styles import getSampleStyleSheet\n",
    "    from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle\n",
    "    REPORTLAB_AVAILABLE = True\n",
    "except Exception:\n",
    "    REPORTLAB_AVAILABLE = False\n",
    "\n",
    "# ---------- LIMITS (safe for driver -> pandas) ----------\n",
    "ROW_CAP_XLSX = 1_000_000   # Excel max is ~1,048,576\n",
    "ROW_CAP_PDF  = 5_000       # keep PDFs small & readable\n",
    "\n",
    "# ---------- PATH HELPERS ----------\n",
    "def dbfs_to_local(dbfs_path: str) -> str:\n",
    "    # 'dbfs:/FileStore/...' -> '/dbfs/FileStore/...'\n",
    "    return \"/dbfs\" + dbfs_path.replace(\"dbfs:\", \"\")\n",
    "\n",
    "def artifact_dir(report_name: str) -> str:\n",
    "    today = datetime.now(timezone.utc).strftime(\"%Y%m%d\")\n",
    "    return f\"dbfs:/FileStore/kyc/reports/{today}/{report_name}\"\n",
    "\n",
    "def files_url(dbfs_path: str) -> str:\n",
    "    # Relative link works in Databricks: '/files/...'\n",
    "    return \"/files/\" + dbfs_path.replace(\"dbfs:/FileStore/\", \"\")\n",
    "\n",
    "def write_text(dbfs_path: str, text: str):\n",
    "    local = dbfs_to_local(dbfs_path)\n",
    "    os.makedirs(os.path.dirname(local), exist_ok=True)\n",
    "    with open(local, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "\n",
    "def write_bytes(dbfs_path: str, data: bytes):\n",
    "    local = dbfs_to_local(dbfs_path)\n",
    "    os.makedirs(os.path.dirname(local), exist_ok=True)\n",
    "    with open(local, \"wb\") as f:\n",
    "        f.write(data)\n",
    "\n",
    "def sha256_bytes(data: bytes) -> str:\n",
    "    import hashlib\n",
    "    return hashlib.sha256(data).hexdigest()\n",
    "\n",
    "def sanitize_identifier(name: str) -> str:\n",
    "    return (\n",
    "        name.lower()\n",
    "        .replace(\"-\", \"_\")\n",
    "        .replace(\" \", \"_\")\n",
    "        .replace(\"/\", \"_\")\n",
    "    )\n",
    "\n",
    "# --- Pre-escape helpers & SQL literals ---\n",
    "def esc(s: str | None) -> str | None:\n",
    "    return s.replace(\"'\", \"''\") if s is not None else None\n",
    "\n",
    "\n",
    "# ---------- LOAD METADATA (single JSON array) ----------\n",
    "def load_metadata_array(meta_file: str):\n",
    "    # read full file (assumed modest size)\n",
    "    txt = dbutils.fs.head(meta_file, 5_000_000)\n",
    "    items = json.loads(txt)\n",
    "    assert isinstance(items, list), \"Metadata must be a JSON array\"\n",
    "    return {m.get(\"report_name\"): m for m in items if m.get(\"report_name\")}\n",
    "    \n",
    "META = load_metadata_array(META_FILE)\n",
    "\n",
    "# ---------- TABLES ----------\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {SCHEMA}\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {CATALOG}.{SCHEMA}.report_candidates (\n",
    "  id STRING,\n",
    "  report_name STRING,\n",
    "  report_owner STRING,\n",
    "  status STRING,\n",
    "  view_name STRING,\n",
    "  export_format STRING,\n",
    "  widget_title STRING,\n",
    "  filters STRING,\n",
    "  report_sql STRING,\n",
    "  draft_manifest_path STRING,\n",
    "  report_url STRING,\n",
    "  dashboard STRING,\n",
    "  notify ARRAY<STRING>,\n",
    "  certify BOOLEAN,\n",
    "  created_at TIMESTAMP,\n",
    "  reviewed_at TIMESTAMP,\n",
    "  reviewed_by STRING,\n",
    "  published_at TIMESTAMP\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41d3d38f-d511-41ec-a985-4ac0baa0dbf1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Exporter\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def export_report(report_name: str, sql_select_all_from_view: str, export_format: str, widget_title: str, filters: str, owner: str | None):\n",
    "    \"\"\"\n",
    "    Runs Spark SQL, emits CSV / XLSX / PDF (optional) to DBFS as _DRAFT artifacts,\n",
    "    and writes a JSON manifest. Returns (draft_paths, manifest_path).\n",
    "    \"\"\"\n",
    "    # 1) Query with Spark SQL\n",
    "    sdf = spark.sql(sql_select_all_from_view)\n",
    "\n",
    "    # collect to pandas only when needed (xlsx/pdf)\n",
    "    # CSV we'll also do via pandas.to_csv for simplicity (single file) — safe for moderate row counts.\n",
    "    pdf = sdf.toPandas()\n",
    "\n",
    "    base_dir = artifact_dir(report_name)\n",
    "    os.makedirs(dbfs_to_local(base_dir), exist_ok=True)\n",
    "    base = f\"{base_dir}/{report_name}_{uuid.uuid4().hex[:8]}\"\n",
    "\n",
    "    draft_paths = []\n",
    "    artifacts = []\n",
    "\n",
    "    targets = ['csv','xlsx','pdf'] if export_format == 'all' else [export_format]\n",
    "\n",
    "    # CSV\n",
    "    if 'csv' in targets:\n",
    "        csv_path = f\"{base}_DRAFT.csv\"\n",
    "        csv_bytes = pdf.to_csv(index=False).encode(\"utf-8\")\n",
    "        write_bytes(csv_path, csv_bytes)\n",
    "        draft_paths.append(csv_path)\n",
    "        artifacts.append({\"type\":\"csv\",\"path\":csv_path,\"sha256\":sha256_bytes(csv_bytes),\"files_url\":files_url(csv_path)})\n",
    "\n",
    "    # XLSX\n",
    "    if 'xlsx' in targets:\n",
    "        lim_pdf = pdf if len(pdf) <= ROW_CAP_XLSX else pdf.iloc[:ROW_CAP_XLSX].copy()\n",
    "        xlsx_path = f\"{base}_DRAFT.xlsx\"\n",
    "        bio = io.BytesIO()\n",
    "        with pd.ExcelWriter(bio, engine=\"xlsxwriter\") as xw:\n",
    "            lim_pdf.to_excel(xw, sheet_name=\"Report\", index=False)\n",
    "            ws = xw.sheets[\"Report\"]\n",
    "            for i, col in enumerate(lim_pdf.columns):\n",
    "                maxlen = max([len(str(x)) for x in [col] + lim_pdf[col].astype(str).tolist()[:100]])\n",
    "                ws.set_column(i, i, min(maxlen + 2, 60))\n",
    "        write_bytes(xlsx_path, bio.getvalue())\n",
    "        draft_paths.append(xlsx_path)\n",
    "        artifacts.append({\"type\":\"xlsx\",\"path\":xlsx_path,\"sha256\":sha256_bytes(bio.getvalue()),\"files_url\":files_url(xlsx_path)})\n",
    "\n",
    "    # PDF (optional)\n",
    "    if 'pdf' in targets:\n",
    "        if not REPORTLAB_AVAILABLE:\n",
    "            print(\"PDF export skipped: reportlab not installed on this cluster.\")\n",
    "        else:\n",
    "            lim_pdf = pdf if len(pdf) <= ROW_CAP_PDF else pdf.iloc[:ROW_CAP_PDF].copy()\n",
    "            pdf_path = f\"{base}_DRAFT.pdf\"\n",
    "            bio = io.BytesIO()\n",
    "            doc = SimpleDocTemplate(bio, pagesize=landscape(A4), leftMargin=24, rightMargin=24, topMargin=24, bottomMargin=24)\n",
    "            styles = getSampleStyleSheet()\n",
    "            elems = [Paragraph(widget_title or report_name, styles['Title']), Spacer(1, 8)]\n",
    "            data = [list(lim_pdf.columns)] + lim_pdf.astype(str).values.tolist()\n",
    "            table = Table(data)\n",
    "            table.setStyle(TableStyle([\n",
    "                ('BACKGROUND', (0,0), (-1,0), colors.lightgrey),\n",
    "                ('TEXTCOLOR', (0,0), (-1,0), colors.black),\n",
    "                ('FONTSIZE', (0,0), (-1,-1), 8),\n",
    "                ('GRID', (0,0), (-1,-1), 0.25, colors.grey),\n",
    "                ('ALIGN', (0,0), (-1,-1), 'LEFT'),\n",
    "            ]))\n",
    "            elems.append(table)\n",
    "            doc.build(elems)\n",
    "            write_bytes(pdf_path, bio.getvalue())\n",
    "            draft_paths.append(pdf_path)\n",
    "            artifacts.append({\"type\":\"pdf\",\"path\":pdf_path,\"sha256\":sha256_bytes(bio.getvalue()),\"files_url\":files_url(pdf_path)})\n",
    "\n",
    "    # manifest\n",
    "    manifest = {\n",
    "        \"id\": str(uuid.uuid4()),\n",
    "        \"report_name\": report_name,\n",
    "        \"export_format\": export_format,\n",
    "        \"row_count\": int(len(pdf)),\n",
    "        \"generated_at\": datetime.now(timezone.utc).isoformat(),\n",
    "        \"owner\": owner,\n",
    "        \"filters\": filters,\n",
    "        \"sql\": sql_select_all_from_view,\n",
    "        \"artifacts\": artifacts,\n",
    "    }\n",
    "    manifest_path = f\"{base}_DRAFT.manifest.json\"\n",
    "    write_text(manifest_path, json.dumps(manifest, indent=2))\n",
    "\n",
    "    return draft_paths, manifest_path, manifest, len(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cb05b1f-00a0-4a4f-9005-cfe71c627835",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Process approved AI SQL → create view → export drafts → upsert tracking\n",
    "\n",
    "# Pull approved candidates\n",
    "approved = spark.sql(f\"\"\"\n",
    "  SELECT id, report_name, generated_sql, created_by\n",
    "  FROM {AI_CANDIDATES_VIEW}\n",
    "  WHERE status = 'APPROVED' \"\"\").collect()\n",
    "\n",
    "if not approved:\n",
    "    print(\"No approved AI SQL candidates found.\")\n",
    "else:\n",
    "    for row in approved:\n",
    "        ai_id       = row[\"id\"]\n",
    "        report_name = row[\"report_name\"]\n",
    "        sql_text    = row[\"generated_sql\"]\n",
    "        owner       = row[\"created_by\"]\n",
    "\n",
    "        print(f\"Report -{report_name}\")\n",
    "        \n",
    "        if not report_name or not sql_text:\n",
    "            print(f\"Skipping id={ai_id}: missing report_name or sql\")\n",
    "            continue\n",
    "\n",
    "        # metadata adornments (format/title/dashboard/notify/certify/filters)\n",
    "        meta = META.get(report_name, {})\n",
    "        export_format = (meta.get(\"export_format\") or \"csv\").lower()\n",
    "        widget_title  = meta.get(\"widget_title\") or report_name\n",
    "        dashboard     = meta.get(\"dashboard\") or None\n",
    "        filters_meta  = meta.get(\"filters\") or \"\"\n",
    "        certify       = bool(meta.get(\"certify\", False))\n",
    "        notify        = meta.get(\"notify\", [])\n",
    "\n",
    "        # 1) Create/replace the UC view from the APPROVED SQL\n",
    "        view_name = sanitize_identifier(f\"vw_{report_name}\")\n",
    "        full_view = f\"{CATALOG}.{SCHEMA}.{view_name}\"\n",
    "        spark.sql(f\"CREATE OR REPLACE VIEW {full_view} AS {sql_text}\")\n",
    "\n",
    "        # annotate view (comments / certification flag)\n",
    "        desc = meta.get(\"description\", \"\")\n",
    "        nl   = meta.get(\"natural_language\", \"\")\n",
    "        comment = f\"{desc} | {nl}\".strip(\" |\")\n",
    "        comment = comment.replace(\"'\", \"''\")\n",
    "        if comment:\n",
    "            spark.sql(\n",
    "                f\"COMMENT ON VIEW {full_view} IS '{comment}'\"\n",
    "            )\n",
    "        if certify:\n",
    "            spark.sql(\n",
    "                f\"ALTER VIEW {full_view} SET TBLPROPERTIES ('quality'='certified')\"\n",
    "            )\n",
    "\n",
    "        print(f\"Created view: {full_view}\")\n",
    "\n",
    "        # 2) Persist DDL + metadata snapshot\n",
    "        ts = datetime.now(timezone.utc).strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "        dump_dir = f\"{VIEW_DUMP_ROOT}/{sanitize_identifier(report_name)}/{ts}_{uuid.uuid4().hex[:6]}\"\n",
    "        ddl = f\"CREATE OR REPLACE VIEW {full_view} AS\\n{sql_text}\\n\"\n",
    "        write_text(f\"{dump_dir}/view.sql\", ddl)\n",
    "        write_text(f\"{dump_dir}/metadata.json\", json.dumps({\"ai_id\": ai_id, \"report_name\": report_name, \"view\": full_view, \"metadata\": meta}, indent=2))\n",
    "\n",
    "        # 3) Generate DRAFT artifacts from the view\n",
    "        draft_paths, manifest_path, manifest, row_count = export_report(\n",
    "            report_name=report_name,\n",
    "            sql_select_all_from_view=f\"SELECT * FROM {full_view}\",\n",
    "            export_format=export_format,\n",
    "            widget_title=widget_title,\n",
    "            filters=filters_meta,\n",
    "            owner=owner\n",
    "        )\n",
    "        print(\"Draft artifacts:\")\n",
    "        for p in draft_paths:\n",
    "            print(\"  \", p)\n",
    "\n",
    "        # 4) Upsert into tracking table (ready_for_business)\n",
    "        if notify:\n",
    "            safe_notify = [n.replace(\"'\", \"''\") for n in notify]\n",
    "            notify_sql = \"ARRAY(\" + \", \".join([f\"'{n}'\" for n in safe_notify]) + \")\"\n",
    "        else:\n",
    "            notify_sql = \"ARRAY()\"\n",
    "\n",
    "        # Optional fields\n",
    "        owner_sql      = f\"'{esc(owner)}'\" if owner else \"NULL\"\n",
    "        dashboard_sql  = f\"'{esc(dashboard)}'\" if dashboard else \"''\"\n",
    "\n",
    "        # Required fields as SQL string literals\n",
    "        ai_id_sql          = f\"'{esc(ai_id)}'\"\n",
    "        report_name_sql    = f\"'{esc(report_name)}'\"\n",
    "        view_name_sql      = f\"'{esc(view_name)}'\"\n",
    "        export_format_sql  = f\"'{esc(export_format)}'\"\n",
    "        widget_title_sql   = f\"'{esc(widget_title)}'\"\n",
    "        filters_sql        = f\"'{esc(filters_meta)}'\"\n",
    "        sql_text_sql       = f\"'{esc(sql_text)}'\"\n",
    "        manifest_path_sql  = f\"'{esc(manifest_path)}'\"\n",
    "\n",
    "        # Booleans\n",
    "        certify_sql = 'true' if certify else 'false'\n",
    "\n",
    "        # Report URL\n",
    "        artifacts = manifest.get(\"artifacts\", [])\n",
    "        report_dbfs_path = artifacts[0][\"path\"] if artifacts else None\n",
    "        report_url = f\"'{files_url(report_dbfs_path)}'\"\n",
    "\n",
    "        # notify_sql already built safely earlier:\n",
    "        # if notify:\n",
    "        #     safe_notify = [n.replace(\"'\", \"''\") for n in notify]\n",
    "        #     notify_sql = \"ARRAY(\" + \", \".join([f\"'{n}'\" for n in safe_notify]) + \")\"\n",
    "        # else:\n",
    "        #     notify_sql = \"ARRAY()\"\n",
    "\n",
    "        # --- Final MERGE ---\n",
    "        spark.sql(f\"\"\"\n",
    "            MERGE INTO {CATALOG}.{SCHEMA}.report_candidates AS t\n",
    "            USING (SELECT {ai_id_sql} AS id) s\n",
    "            ON t.id = s.id\n",
    "            WHEN MATCHED THEN UPDATE SET\n",
    "            report_name         = {report_name_sql},\n",
    "            report_owner               = {owner_sql},\n",
    "            status              = 'ready_for_business',\n",
    "            view_name           = {view_name_sql},\n",
    "            export_format       = {export_format_sql},\n",
    "            widget_title        = {widget_title_sql},\n",
    "            filters             = {filters_sql},\n",
    "            report_sql          = {sql_text_sql},\n",
    "            draft_manifest_path = {manifest_path_sql},\n",
    "            report_url          = {report_url},\n",
    "            dashboard           = {dashboard_sql},\n",
    "            notify              = {notify_sql},\n",
    "            certify             = {certify_sql},\n",
    "            created_at          = current_timestamp()\n",
    "            WHEN NOT MATCHED THEN INSERT (\n",
    "            id, report_name, report_owner, status, view_name, export_format, widget_title, filters,\n",
    "            report_sql, draft_manifest_path, report_url, dashboard, notify, certify, created_at\n",
    "            ) VALUES (\n",
    "            {ai_id_sql}, {report_name_sql}, {owner_sql}, 'ready_for_business', {view_name_sql}, {export_format_sql}, {widget_title_sql}, {filters_sql},\n",
    "            {sql_text_sql}, {manifest_path_sql}, {report_url}, {dashboard_sql}, {notify_sql}, {certify_sql}, current_timestamp()\n",
    "            )\n",
    "        \"\"\")\n",
    "\n",
    "print(\"Done: processed approved AI SQL candidates.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "05_publish_approved_views_advaced",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
