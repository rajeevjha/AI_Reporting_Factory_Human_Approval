{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "398adb90-e906-42ec-9f0c-b81047466a1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Databricks notebook: 06_publish_export_notify_reports\n",
    "## Purpose:\n",
    "For each APPROVED report in finance.kyc_ml.report_candidates   \n",
    "- Move DRAFT files to final (remove \"_DRAFT\" in file name)\n",
    "- Update final_paths, report_url  \n",
    "- Set status='published', published_at=now()\n",
    "- Notify stakeholders (Slack webhook or email stub)\n",
    "## Assumptions: \n",
    "- DRAFT file paths are stored as dbfs:/FileStore/... (or convertible from /files/...)\n",
    "- Running inside a Databricks cluster (dbutils available) for file moves\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18c0c2d7-a5f9-4c47-a1d4-28417ff25bda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os, json, requests\n",
    "from datetime import datetime\n",
    "\n",
    "# ---------------- Widgets / Config ----------------\n",
    "# ---------- CONFIG ----------\n",
    "CATALOG        = \"finance\"\n",
    "SCHEMA         = \"kyc_gold\"\n",
    "META_FILE      = \"dbfs:/FileStore/kyc/report_metadata/report_definitions.json\"\n",
    "VIEW_DUMP_ROOT = \"dbfs:/FileStore/kyc/reports/views\"\n",
    "REPORT_CAND = f\"{CATALOG}.{SCHEMA}.report_candidates\"\n",
    "\n",
    "SLACK_SCOPE = \"\"\n",
    "SLACK_KEY   = \"\"\n",
    "DEFAULT_NOTIFY_EMAIL = \"\"\n",
    "\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "spark.sql(f\"USE SCHEMA {SCHEMA}\")\n",
    "\n",
    "# ---------------- Helpers ----------------\n",
    "def to_dbfs_path(p: str) -> str:\n",
    "    \"\"\"\n",
    "    Accepts '/files/...', 'https://<host>/files/...', or 'dbfs:/FileStore/...'\n",
    "    Returns 'dbfs:/FileStore/...'\n",
    "    \"\"\"\n",
    "    q = (p or \"\").strip()\n",
    "    if not q:\n",
    "        return q\n",
    "    if q.startswith(\"http://\") or q.startswith(\"https://\"):\n",
    "        # keep only path part after the host\n",
    "        parts = q.split(\"/\", 3)\n",
    "        q = \"/\" + parts[3] if len(parts) >= 4 else \"/\"\n",
    "    if q.startswith(\"/files/\"):\n",
    "        return \"dbfs:/FileStore/\" + q[len(\"/files/\"):]\n",
    "    if q.startswith(\"dbfs:/FileStore/\"):\n",
    "        return q\n",
    "    if q.startswith(\"dbfs:/\"):\n",
    "        return q\n",
    "    # last resort: treat as relative under FileStore\n",
    "    return \"dbfs:/FileStore/\" + q.lstrip(\"/\")\n",
    "\n",
    "def strip_draft_suffix(dbfs_path: str) -> str:\n",
    "    # ..._DRAFT.ext -> ... .ext (single occurrence before last extension)\n",
    "    return dbfs_path.replace(\"_DRAFT.\", \".\")\n",
    "\n",
    "def dbfs_to_files_url(dbfs_path: str) -> str:\n",
    "    # Resolve workspace host for clickable URL\n",
    "    host_no_scheme = os.getenv('DATABRICKS_HOST', '').replace('https://', '').replace('http://', '')\n",
    "    return f\"https://{host_no_scheme}/files/{dbfs_path.replace('dbfs:/FileStore/','')}\"\n",
    "\n",
    "def get_slack_webhook() -> str | None:\n",
    "    if SLACK_SCOPE and SLACK_KEY:\n",
    "        try:\n",
    "            return dbutils.secrets.get(scope=SLACK_SCOPE, key=SLACK_KEY)  # type: ignore\n",
    "        except Exception:\n",
    "            return None\n",
    "    return os.getenv(\"SLACK_WEBHOOK_URL\")\n",
    "\n",
    "def notify_stakeholders(report_row: dict, report_url: str):\n",
    "    \"\"\"\n",
    "    Priority: Slack webhook (if configured). Otherwise print/email stub.\n",
    "    Expects report_row to have keys: report_name, owner, notify (array-like), description\n",
    "    \"\"\"\n",
    "    title = report_row.get(\"report_name\", \"Report\")\n",
    "    owner = report_row.get(\"owner\", \"\")\n",
    "    notify = report_row.get(\"notify\") or []\n",
    "    if isinstance(notify, str):\n",
    "        # handle serialized arrays\n",
    "        try:\n",
    "            import ast\n",
    "            notify = ast.literal_eval(notify)\n",
    "        except Exception:\n",
    "            notify = [notify] if notify else []\n",
    "\n",
    "    # Slack\n",
    "    webhook = get_slack_webhook()\n",
    "    if webhook:\n",
    "        text = f\"*{title}* is now *PUBLISHED*.\\n<{report_url}|Open report file>\\nOwner: `{owner}`\"\n",
    "        try:\n",
    "            r = requests.post(webhook, json={\"text\": text}, timeout=15)\n",
    "            r.raise_for_status()\n",
    "            print(\"Sent Slack notification.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Slack notify failed: {e}\")\n",
    "\n",
    "    # Email stub or fallback print\n",
    "    if notify or DEFAULT_NOTIFY_EMAIL:\n",
    "        recipients = notify or [DEFAULT_NOTIFY_EMAIL]\n",
    "        # Implement your SMTP / Email integration here.\n",
    "        # For now, we log intended recipients.\n",
    "        print(f\"Email notify ‚Üí {', '.join([str(x) for x in recipients if x])} :: {report_url}\")\n",
    "    else:\n",
    "        print(\"No email recipients configured (notify empty & no DEFAULT_NOTIFY_EMAIL).\")\n",
    "\n",
    "# ---------------- 1) Pick APPROVED reports ----------------\n",
    "approved = spark.sql(f\"\"\"\n",
    "  SELECT id, report_name, view_name, report_owner, export_format, report_url, notify\n",
    "  FROM {REPORT_CAND}\n",
    "  WHERE status = 'APPROVED'\n",
    "\"\"\").collect()\n",
    "\n",
    "print(f\"Approved reports to publish: {len(approved)}\")\n",
    "if not approved:\n",
    "    dbutils.notebook.exit(\"No approved reports to publish.\")  # graceful exit\n",
    "\n",
    "# ---------------- 2) Finalize drafts ‚Üí publish & update table ----------------\n",
    "for r in approved:\n",
    "    rid = r[\"id\"]\n",
    "    report_name = r[\"report_name\"]\n",
    "    export_format = (r[\"export_format\"] or \"xlsx\").lower()\n",
    "    draft_paths = r[\"report_url\"] or []\n",
    "\n",
    "    # Some drivers serialize array<STRING> ‚Äì normalize to list[str]\n",
    "    if isinstance(draft_paths, str):\n",
    "        try:\n",
    "            import ast\n",
    "            draft_paths = ast.literal_eval(draft_paths)\n",
    "        except Exception:\n",
    "            draft_paths = [draft_paths]\n",
    "\n",
    "    if not draft_paths:\n",
    "        print(f\"‚ö†Ô∏è No report_url for report {report_name} (id={rid}); skipping.\")\n",
    "        continue\n",
    "\n",
    "    final_paths = []\n",
    "    for p in draft_paths:\n",
    "        src_dbfs = to_dbfs_path(p)\n",
    "        dst_dbfs = strip_draft_suffix(src_dbfs)\n",
    "\n",
    "        # Ensure parent exists (dbutils will create as needed, but mkdirs for safety)\n",
    "        try:\n",
    "            parent = \"/\".join(dst_dbfs.split(\"/\")[:-1])\n",
    "            if parent:\n",
    "                dbutils.fs.mkdirs(parent)  # type: ignore\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            # Overwrite if exists: delete and move\n",
    "            try:\n",
    "                dbutils.fs.rm(dst_dbfs, recurse=False)  # type: ignore\n",
    "            except Exception:\n",
    "                pass\n",
    "            dbutils.fs.mv(src_dbfs, dst_dbfs, True)  # type: ignore\n",
    "            final_paths.append(dst_dbfs)\n",
    "            print(f\"Published: {src_dbfs} ‚Üí {dst_dbfs}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to publish {src_dbfs}: {e}\")\n",
    "            # Continue to next file, do not stop whole run\n",
    "\n",
    "    if not final_paths:\n",
    "        print(f\"‚ö†Ô∏è No files published for report {report_name} (id={rid}); leaving status approved.\")\n",
    "        continue\n",
    "\n",
    "    # Canonical URL for the first final file\n",
    "    report_url = dbfs_to_files_url(final_paths[0])\n",
    "\n",
    "    # Update row ‚Üí published\n",
    "    spark.sql(f\"\"\"\n",
    "      UPDATE {REPORT_CAND}\n",
    "      SET\n",
    "          report_url = '{report_url}',\n",
    "          status = 'published',\n",
    "          published_at = current_timestamp()\n",
    "      WHERE id = '{rid}'\n",
    "    \"\"\")\n",
    "    print(f\"‚úÖ Report published: {report_name} (id={rid}) ‚Üí {report_url}\")\n",
    "\n",
    "    # Notify stakeholders\n",
    "    # notify_stakeholders({\n",
    "    #   \"report_name\": report_name,\n",
    "    #    \"owner\": r[\"owner\"],\n",
    "    #    \"notify\": r[\"notify\"]\n",
    "    # }, report_url)\n",
    "\n",
    "print(\"üéØ Done: Published all APPROVED reports, updated to PUBLISHED, and sent notifications where configured.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "06_report_export_notify",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
