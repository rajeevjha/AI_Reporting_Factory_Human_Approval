{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "398adb90-e906-42ec-9f0c-b81047466a1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Databricks notebook: 06_publish_export_notify_reports\n",
    "## Purpose:\n",
    "For each APPROVED report in finance.kyc_ml.report_candidates   \n",
    "- Move DRAFT files to final (remove \"_DRAFT\" in file name)\n",
    "- Update final_paths, report_url  \n",
    "- Set status='published', published_at=now()\n",
    "- Notify stakeholders (Slack webhook or email stub)\n",
    "## Assumptions: \n",
    "- DRAFT file paths are stored as dbfs:/FileStore/... (or convertible from /files/...)\n",
    "- Running inside a Databricks cluster (dbutils available) for file moves\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18c0c2d7-a5f9-4c47-a1d4-28417ff25bda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os, json, requests\n",
    "from datetime import datetime\n",
    "\n",
    "# ---------------- Widgets / Config ----------------\n",
    "# ---------- CONFIG ----------\n",
    "CATALOG        = \"finance\"\n",
    "SCHEMA         = \"kyc_gold\"\n",
    "META_FILE      = \"dbfs:/FileStore/kyc/report_metadata/report_definitions.json\"\n",
    "VIEW_DUMP_ROOT = \"dbfs:/FileStore/kyc/reports/views\"\n",
    "REPORT_CAND = f\"{CATALOG}.{SCHEMA}.report_candidates\"\n",
    "\n",
    "DEFAULT_NOTIFY_EMAIL = \"\"\n",
    "\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "spark.sql(f\"USE SCHEMA {SCHEMA}\")\n",
    "\n",
    "# ---------------- Helpers ----------------\n",
    "def to_dbfs_path(p: str) -> str:\n",
    "    \"\"\"\n",
    "    Accepts '/files/...', 'https://<host>/files/...', or 'dbfs:/FileStore/...'\n",
    "    Returns 'dbfs:/FileStore/...'\n",
    "    \"\"\"\n",
    "    q = (p or \"\").strip()\n",
    "    if not q:\n",
    "        return q\n",
    "    if q.startswith(\"http://\") or q.startswith(\"https://\"):\n",
    "        # keep only path part after the host\n",
    "        parts = q.split(\"/\", 3)\n",
    "        q = \"/\" + parts[3] if len(parts) >= 4 else \"/\"\n",
    "    if q.startswith(\"/files/\"):\n",
    "        return \"dbfs:/FileStore/\" + q[len(\"/files/\"):]\n",
    "    if q.startswith(\"dbfs:/FileStore/\"):\n",
    "        return q\n",
    "    if q.startswith(\"dbfs:/\"):\n",
    "        return q\n",
    "    # last resort: treat as relative under FileStore\n",
    "    return \"dbfs:/FileStore/\" + q.lstrip(\"/\")\n",
    "\n",
    "def strip_draft_suffix(dbfs_path: str) -> str:\n",
    "    # ..._DRAFT.ext -> ... .ext (single occurrence before last extension)\n",
    "    return dbfs_path.replace(\"_DRAFT.\", \".\")\n",
    "\n",
    "def dbfs_to_files_url(dbfs_path: str) -> str:\n",
    "    # dbfs:/FileStore/...  -> https://<host>/files/...\n",
    "    host = \"https://adb-4274438097098742.2.azuredatabricks.net/\"\n",
    "    suffix = dbfs_path.replace(\"dbfs:/FileStore/\",\"files\")\n",
    "    return f\"{host}/{suffix}\"\n",
    "\n",
    "def notify_stakeholders(report_row: dict, report_url: str):\n",
    "    title = report_row.get(\"report_name\", \"Report\")\n",
    "    owner = report_row.get(\"owner\", \"\")\n",
    "    notify = report_row.get(\"notify\") or []\n",
    "\n",
    "    # Email\n",
    "    if notify or DEFAULT_NOTIFY_EMAIL:\n",
    "        recipients = notify or [DEFAULT_NOTIFY_EMAIL]\n",
    "        # Implement your SMTP / Email integration here.\n",
    "\n",
    "        api_key = dbutils.secrets.get(\"email-secrets\", \"GRAPH_CLIENT_SECRET\") #MAILERSEND_API_KEY\n",
    "\n",
    "        url = \"https://api.mailersend.com/v1/email\"\n",
    "        payload = {\n",
    "            \"from\": {\"email\": \"MS_Wp3pH3@test-nrw7gymdorog2k8e.mlsender.net\"},  \n",
    "            \"to\": [{\"email\": addr} for addr in recipients if addr],\n",
    "            \"subject\": f\"Report '{title}'is ready\",\n",
    "            \"text\": f\"Hi,<p>Report '{title}' is available to download at {report_url}</p>\"\n",
    "        }\n",
    "\n",
    "        resp = requests.post(\n",
    "            url,\n",
    "            headers={\"Authorization\": f\"Bearer {api_key}\", \"Content-Type\": \"application/json\"},\n",
    "            data=json.dumps(payload)\n",
    "        )\n",
    "\n",
    "        # For now, we log intended recipients.\n",
    "        print(f\"Email notified ‚Üí {', '.join([str(x) for x in recipients if x])} :: {report_url}\")\n",
    "    else:\n",
    "        print(\"No email recipients configured (notify empty & no DEFAULT_NOTIFY_EMAIL).\")\n",
    "\n",
    "# ---------------- 1) Pick APPROVED reports ----------------\n",
    "approved = spark.sql(f\"\"\"\n",
    "  SELECT id, report_name, view_name, report_owner, export_format, report_url, notify\n",
    "  FROM {REPORT_CAND}\n",
    "  WHERE status = 'APPROVED'\n",
    "\"\"\").collect()\n",
    "\n",
    "print(f\"Approved reports to publish: {len(approved)}\")\n",
    "if not approved:\n",
    "    dbutils.notebook.exit(\"No approved reports to publish.\")  # graceful exit\n",
    "\n",
    "# ---------------- 2) Finalize drafts ‚Üí publish & update table ----------------\n",
    "for r in approved:\n",
    "    rid = r[\"id\"]\n",
    "    report_name = r[\"report_name\"]\n",
    "    export_format = (r[\"export_format\"] or \"xlsx\").lower()\n",
    "    draft_paths = r[\"report_url\"] or []\n",
    "    report_owner = r[\"report_owner\"] or []\n",
    "    notify  = r[\"notify\"] or []\n",
    "    view_name = r[\"view_name\"]\n",
    "\n",
    "    # Some drivers serialize array<STRING> ‚Äì normalize to list[str]\n",
    "    if isinstance(draft_paths, str):\n",
    "        try:\n",
    "            import ast\n",
    "            draft_paths = ast.literal_eval(draft_paths)\n",
    "        except Exception:\n",
    "            draft_paths = [draft_paths]\n",
    "\n",
    "    if not draft_paths:\n",
    "        print(f\"‚ö†Ô∏è No report_url for report {report_name} (id={rid}); skipping.\")\n",
    "        continue\n",
    "\n",
    "    final_paths = []\n",
    "    for p in draft_paths:\n",
    "        src_dbfs = to_dbfs_path(p)\n",
    "        dst_dbfs = strip_draft_suffix(src_dbfs)\n",
    "\n",
    "        # Ensure parent exists (dbutils will create as needed, but mkdirs for safety)\n",
    "        try:\n",
    "            parent = \"/\".join(dst_dbfs.split(\"/\")[:-1])\n",
    "            if parent:\n",
    "                dbutils.fs.mkdirs(parent)  # type: ignore\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            # Overwrite if exists: delete and move\n",
    "            try:\n",
    "                dbutils.fs.rm(dst_dbfs, recurse=False)  # type: ignore\n",
    "            except Exception:\n",
    "                pass\n",
    "            dbutils.fs.mv(src_dbfs, dst_dbfs, True)  # type: ignore\n",
    "            final_paths.append(dst_dbfs)\n",
    "            print(f\"Published: {src_dbfs} ‚Üí {dst_dbfs}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to publish {src_dbfs}: {e}\")\n",
    "            # Continue to next file, do not stop whole run\n",
    "\n",
    "    if not final_paths:\n",
    "        print(f\"‚ö†Ô∏è No files published for report {report_name} (id={rid}); leaving status approved.\")\n",
    "        continue\n",
    "\n",
    "    # Canonical URL for the first final file\n",
    "    report_url = dbfs_to_files_url(final_paths[0])\n",
    "\n",
    "    # Update row ‚Üí published\n",
    "    spark.sql(f\"\"\"\n",
    "      UPDATE {REPORT_CAND}\n",
    "      SET\n",
    "          report_url = '{report_url}',\n",
    "          status = 'published',\n",
    "          published_at = current_timestamp()\n",
    "      WHERE id = '{rid}'\n",
    "    \"\"\")\n",
    "    print(f\"‚úÖ Report published: {report_name} (id={rid}) ‚Üí {report_url}\")\n",
    "\n",
    "    # Notify stakeholders\n",
    "    notify_stakeholders({\n",
    "       \"report_name\": report_name,\n",
    "        \"owner\": r[\"report_owner\"],\n",
    "        \"notify\": r[\"notify\"]\n",
    "    }, report_url)\n",
    "\n",
    "print(\"üéØ Done: Published all APPROVED reports, updated to PUBLISHED, and sent notifications where configured.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "06_report_export_notify",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
